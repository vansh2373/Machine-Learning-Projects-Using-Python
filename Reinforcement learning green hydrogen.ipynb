{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b95f0496",
   "metadata": {},
   "source": [
    "# Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a479cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NumPy library for numerical operations, including array manipulations and mathematical functions\n",
    "import numpy as np\n",
    "# Import the Pandas library for handling and analyzing structured data (e.g., CSV, DataFrames)\n",
    "import pandas as pd\n",
    "# Import the random module to generate random numbers, useful for stochastic processes in simulations\n",
    "import random\n",
    "# Import Matplotlib for data visualization, particularly for creating graphs and plots\n",
    "import matplotlib.pyplot as plt\n",
    "# Import defaultdict from the collections module to create dictionary-like objects \n",
    "# with a default value for missing keys (useful in reinforcement learning and Q-learning)\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f17ba",
   "metadata": {},
   "source": [
    "# Step 2: Generate Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd13bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Voltage (V)</th>\n",
       "      <th>Temperature (¬∞C)</th>\n",
       "      <th>Pressure (bar)</th>\n",
       "      <th>Energy Consumption (kWh/kg H‚ÇÇ)</th>\n",
       "      <th>Hydrogen Yield (kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.874540</td>\n",
       "      <td>70.944851</td>\n",
       "      <td>1.740532</td>\n",
       "      <td>-3.325001</td>\n",
       "      <td>23.740503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.450714</td>\n",
       "      <td>66.082891</td>\n",
       "      <td>3.167604</td>\n",
       "      <td>2.951661</td>\n",
       "      <td>25.202058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.231994</td>\n",
       "      <td>59.285828</td>\n",
       "      <td>4.491783</td>\n",
       "      <td>-2.565596</td>\n",
       "      <td>23.168968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.098658</td>\n",
       "      <td>74.413851</td>\n",
       "      <td>3.928900</td>\n",
       "      <td>-8.848814</td>\n",
       "      <td>26.464273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.656019</td>\n",
       "      <td>70.541935</td>\n",
       "      <td>4.226245</td>\n",
       "      <td>0.041591</td>\n",
       "      <td>26.712804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Voltage (V)  Temperature (¬∞C)  Pressure (bar)  \\\n",
       "0     1.874540         70.944851        1.740532   \n",
       "1     2.450714         66.082891        3.167604   \n",
       "2     2.231994         59.285828        4.491783   \n",
       "3     2.098658         74.413851        3.928900   \n",
       "4     1.656019         70.541935        4.226245   \n",
       "\n",
       "   Energy Consumption (kWh/kg H‚ÇÇ)  Hydrogen Yield (kg)  \n",
       "0                       -3.325001            23.740503  \n",
       "1                        2.951661            25.202058  \n",
       "2                       -2.565596            23.168968  \n",
       "3                       -8.848814            26.464273  \n",
       "4                        0.041591            26.712804  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate synthetic hydrogen production data\n",
    "np.random.seed(42)\n",
    "# Setting a seed ensures that the random numbers generated are the same every time the code is executed.\n",
    "# This helps in reproducibility.\n",
    "\n",
    "# Generate 500 samples\n",
    "data_size = 500\n",
    "# This variable represents the number of synthetic data points to be generated (500 samples).\n",
    "\n",
    "# Random values for electrolysis parameters\n",
    "# The np.random.uniform(a, b, size) function generates data_size number of values between a and b.\n",
    "voltage = np.random.uniform(1.5, 2.5, data_size)  # Voltage (V)\n",
    "temperature = np.random.uniform(50, 80, data_size)  # Temperature (¬∞C)\n",
    "pressure = np.random.uniform(1, 5, data_size)  # Pressure (bar)\n",
    "\n",
    "# Energy consumption (lower is better)\n",
    "energy_consumption = 50 - (voltage * 5 + temperature * 0.5 + pressure * 2) + np.random.normal(0, 2, data_size)\n",
    "\n",
    "# Hydrogen yield (higher is better)\n",
    "hydrogen_yield = (voltage * 3 + temperature * 0.2 + pressure * 1.5) + np.random.normal(0, 1, data_size)\n",
    "\n",
    "# Creating DataFrame\n",
    "# Stores all electrolysis parameters, energy consumption, and hydrogen yield in a structured format.\n",
    "# Easier to analyze, visualize, and process the data.\n",
    "df = pd.DataFrame({\n",
    "    'Voltage (V)': voltage,\n",
    "    'Temperature (¬∞C)': temperature,\n",
    "    'Pressure (bar)': pressure,\n",
    "    'Energy Consumption (kWh/kg H‚ÇÇ)': energy_consumption,\n",
    "    'Hydrogen Yield (kg)': hydrogen_yield\n",
    "})\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd6be6",
   "metadata": {},
   "source": [
    "üîπ Explanation:\n",
    "\n",
    "We simulate 500 observations of electrolysis conditions.\n",
    "\n",
    "Energy consumption is inversely related to efficiency (lower is better).\n",
    "\n",
    "Hydrogen yield is directly proportional to optimal electrolysis settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d8c4e0",
   "metadata": {},
   "source": [
    "# Step 3: Define the Reinforcement Learning Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0080ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Python class HydrogenOptimizationEnv defines a Reinforcement Learning (RL) environment\n",
    "# where an agent (AI model) can adjust voltage, temperature, and pressure to optimize hydrogen production.\n",
    "# The AI agent will interact with this environment by adjusting voltage, temperature, and pressure.\n",
    "class HydrogenOptimizationEnv:\n",
    "    def __init__(self):\n",
    "        # This constructor method (__init__) initializes the environment when an object of the class is created.\n",
    "# It defines the state space, action space, and the initial state.\n",
    "        # Define state space (Voltage, Temperature, Pressure)\n",
    "        self.state_space = {\n",
    "            'voltage': np.linspace(1.5, 2.5, 10),  # 10 voltage levels\n",
    "            'temperature': np.linspace(50, 80, 10),  # 10 temperature levels\n",
    "            'pressure': np.linspace(1, 5, 5)  # 5 pressure levels\n",
    "        }\n",
    "        \n",
    "        # Define actions (increase or decrease each parameter)\n",
    "        self.action_space = ['increase_voltage', 'decrease_voltage',\n",
    "                             'increase_temp', 'decrease_temp',\n",
    "                             'increase_pressure', 'decrease_pressure']\n",
    "        \n",
    "        # Initial state\n",
    "        self.state = [2.0, 65, 3]  # Starting with an average setting\n",
    "        self.steps = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" Take an action and return new state, reward, and done flag \"\"\"\n",
    "        voltage, temperature, pressure = self.state\n",
    "        \n",
    "        # Apply actions\n",
    "        if action == 'increase_voltage': voltage = min(voltage + 0.1, 2.5)\n",
    "        elif action == 'decrease_voltage': voltage = max(voltage - 0.1, 1.5)\n",
    "        elif action == 'increase_temp': temperature = min(temperature + 2, 80)\n",
    "        elif action == 'decrease_temp': temperature = max(temperature - 2, 50)\n",
    "        elif action == 'increase_pressure': pressure = min(pressure + 0.5, 5)\n",
    "        elif action == 'decrease_pressure': pressure = max(pressure - 0.5, 1)\n",
    "        \n",
    "        # Calculate efficiency: reward is high when hydrogen yield is high & energy consumption is low\n",
    "        hydrogen_yield = (voltage * 3 + temperature * 0.2 + pressure * 1.5)\n",
    "        energy_efficiency = 50 - (voltage * 5 + temperature * 0.5 + pressure * 2)\n",
    "        \n",
    "        reward = hydrogen_yield - (energy_efficiency / 10)  # Balancing both factors\n",
    "        \n",
    "        # Update state\n",
    "        self.state = [voltage, temperature, pressure]\n",
    "        \n",
    "        # End after 50 steps\n",
    "        self.steps += 1\n",
    "        done = self.steps >= 50\n",
    "        \n",
    "        return self.state, reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Reset the environment \"\"\"\n",
    "        self.state = [2.0, 65, 3]  # Reset to initial condition\n",
    "        self.steps = 0\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc106c",
   "metadata": {},
   "source": [
    "State space defines all possible values that the environment can take.\n",
    "\n",
    "We use NumPy‚Äôs linspace function to generate evenly spaced values.\n",
    "\n",
    "Voltage: Ranges from 1.5V to 2.5V, with 10 discrete levels.\n",
    "\n",
    "Temperature: Ranges from 50¬∞C to 80¬∞C, with 10 discrete levels.\n",
    "\n",
    "Pressure: Ranges from 1 bar to 5 bar, with 5 discrete levels.\n",
    "\n",
    "üìå Purpose: This allows our RL agent to pick an optimal combination of these parameters for hydrogen production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc12844",
   "metadata": {},
   "source": [
    "Action space defines what the AI agent can do in the environment.\n",
    "\n",
    "The AI can increase or decrease voltage, temperature, or pressure.\n",
    "\n",
    "These six actions are like buttons that the AI can press to optimize hydrogen yield and energy efficiency.\n",
    "\n",
    "üìå Purpose: The AI will try different actions and learn from experience which actions result in the best hydrogen production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a9eeb",
   "metadata": {},
   "source": [
    "Initial state starts with:\n",
    "\n",
    "Voltage = 2.0V (mid-range)\n",
    "\n",
    "Temperature = 65¬∞C (mid-range)\n",
    "\n",
    "Pressure = 3 bar (mid-range)\n",
    "\n",
    "self.steps = 0 keeps track of how many actions the agent has taken.\n",
    "\n",
    "üìå Purpose: AI starts from an average condition and learns to find better settings through trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eee260",
   "metadata": {},
   "source": [
    "This method allows the AI agent to take an action and update the state (voltage, temperature, pressure) accordingly.\n",
    "\n",
    "The function returns:\n",
    "    \n",
    "New state: The updated values of voltage, temperature, and pressure.\n",
    "    \n",
    "Reward: A score that tells the AI how good or bad the new state is.\n",
    "    \n",
    "Done flag: Tells if the episode is over.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c51302",
   "metadata": {},
   "source": [
    "voltage, temperature, pressure = self.state\n",
    "\n",
    "Reads the current values of voltage, temperature, and pressure.\n",
    "\n",
    "These values will be modified based on the action chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c574d78c",
   "metadata": {},
   "source": [
    "üîπ Explanation:\n",
    "\n",
    "This simulates an electrolysis environment where the RL agent adjusts voltage, temperature, and pressure.\n",
    "\n",
    "Rewards are higher for maximizing hydrogen yield and minimizing energy waste.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c0136",
   "metadata": {},
   "source": [
    "if action == 'increase_voltage': voltage = min(voltage + 0.1, 2.5)\n",
    "elif action == 'decrease_voltage': voltage = max(voltage - 0.1, 1.5)\n",
    "elif action == 'increase_temp': temperature = min(temperature + 2, 80)\n",
    "elif action == 'decrease_temp': temperature = max(temperature - 2, 50)\n",
    "elif action == 'increase_pressure': pressure = min(pressure + 0.5, 5)\n",
    "elif action == 'decrease_pressure': pressure = max(pressure - 0.5, 1)\n",
    "\n",
    "The AI agent picks an action, and this section updates the voltage, temperature, or pressure.\n",
    "Boundary Conditions:\n",
    "    \n",
    "min() ensures values do not exceed the maximum.\n",
    "\n",
    "max() ensures values do not go below the minimum.\n",
    "\n",
    "üìå Purpose: Prevents unrealistic settings like voltage exceeding 2.5V or temperature going below 50¬∞C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efde93d",
   "metadata": {},
   "source": [
    "Summary of What This Code Does\n",
    "\n",
    "Component\t    Explanation\n",
    "\n",
    "State Space\t    Possible values for voltage, temperature, and pressure\n",
    "\n",
    "Action Space\tThe AI can increase/decrease voltage, temperature, and pressure\n",
    "\n",
    "Initial State\tStarts from a mid-range setting\n",
    "\n",
    "Step Function\tThe AI chooses an action, and the state updates accordingly\n",
    "\n",
    "Boundary Checks\tPrevents values from going out of range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3332804",
   "metadata": {},
   "source": [
    "üîπ Example Walkthrough\n",
    "\n",
    "Imagine the AI starts at:\n",
    "\n",
    "Voltage = 2.0V\n",
    "Temperature = 65¬∞C\n",
    "Pressure = 3 bar\n",
    "\n",
    "Scenario 1: AI chooses increase_voltage\n",
    "New voltage: 2.0V + 0.1V = 2.1V\n",
    "Temperature & Pressure remain the same.\n",
    "\n",
    "Scenario 2: AI chooses increase_temp\n",
    "New temperature: 65¬∞C + 2¬∞C = 67¬∞C\n",
    "Voltage & Pressure remain the same.\n",
    "\n",
    "Scenario 3: AI chooses decrease_pressure\n",
    "New pressure: 3 bar - 0.5 bar = 2.5 bar\n",
    "Voltage & Temperature remain the same.\n",
    "\n",
    "üìå AI will repeat this process multiple times to find the best settings for maximizing hydrogen yield while minimizing energy consumption. üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754368f",
   "metadata": {},
   "source": [
    "hydrogen_yield = (voltage * 3 + temperature * 0.2 + pressure * 1.5)\n",
    "\n",
    "What this does:\n",
    "\n",
    "Higher voltage, temperature, and pressure result in more hydrogen production.\n",
    "\n",
    "The formula assigns different weights to each factor:\n",
    "\n",
    "Voltage is the most important (multiplied by 3).\n",
    "\n",
    "Temperature has a smaller impact (multiplied by 0.2).\n",
    "\n",
    "Pressure also has a strong effect (multiplied by 1.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d344e167",
   "metadata": {},
   "source": [
    "energy_efficiency = 50 - (voltage * 5 + temperature * 0.5 + pressure * 2)\n",
    "\n",
    "What this does:\n",
    "    \n",
    "The goal is to minimize energy consumption while maximizing hydrogen production.\n",
    "\n",
    "The higher the voltage, temperature, or pressure, the more energy is used, which is bad.\n",
    "\n",
    "This formula reduces energy efficiency based on these parameters.\n",
    "\n",
    "üîπ Conclusion: If energy efficiency drops too much, it means the AI is using too much energy, which is bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141636c2",
   "metadata": {},
   "source": [
    "reward = hydrogen_yield - (energy_efficiency / 10)  # Balancing both factors\n",
    "\n",
    "What this does:\n",
    "\n",
    "The reward is the final score for the AI's action.\n",
    "\n",
    "The formula ensures hydrogen production is high and energy usage is low.\n",
    "\n",
    "energy_efficiency / 10 ensures that high energy usage slightly reduces the reward but does not completely dominate the hydrogen yield.\n",
    "\n",
    "üîπ Conclusion: The higher the reward, the better the AI's action was."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19caca5e",
   "metadata": {},
   "source": [
    "self.state = [voltage, temperature, pressure]\n",
    "\n",
    "Stores the new values of voltage, temperature, and pressure so that in the next step, the AI starts from these values.\n",
    "\n",
    "This allows the AI to learn over time which settings are best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0172215",
   "metadata": {},
   "source": [
    "self.steps += 1\n",
    "\n",
    "done = self.steps >= 50\n",
    "\n",
    "Each time the AI takes an action, it counts as 1 step.\n",
    "\n",
    "If the AI reaches 50 steps, the experiment ends (done = True).\n",
    "\n",
    "üîπ Conclusion: This ensures the AI does not run forever and must find the best settings within 50 moves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a91d3b9",
   "metadata": {},
   "source": [
    "def reset(self):\n",
    "    \"\"\" Reset the environment \"\"\"\n",
    "    self.state = [2.0, 65, 3]  # Reset to initial condition\n",
    "    self.steps = 0\n",
    "    return self.state\n",
    "\n",
    "Resets the environment back to its starting conditions so the AI can try again.\n",
    "\n",
    "Ensures fair training, as the AI always starts from the same point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9424160",
   "metadata": {},
   "source": [
    "# Step 4: Implement Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5e4f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "Q_table = defaultdict(lambda: np.zeros(len(HydrogenOptimizationEnv().action_space)))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "env = HydrogenOptimizationEnv()\n",
    "\n",
    "# Training\n",
    "for episode in range(1000):  # Train for 1000 episodes\n",
    "    state = tuple(env.reset())\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(env.action_space)  # Explore\n",
    "        else:\n",
    "            action = env.action_space[np.argmax(Q_table[state])]  # Exploit\n",
    "        \n",
    "        new_state, reward, done = env.step(action)\n",
    "        new_state = tuple(new_state)\n",
    "        \n",
    "        # Q-learning update\n",
    "        Q_table[state][env.action_space.index(action)] += alpha * (reward + gamma * np.max(Q_table[new_state]) - Q_table[state][env.action_space.index(action)])\n",
    "        \n",
    "        state = new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bc41c4",
   "metadata": {},
   "source": [
    "üîπ Explanation:\n",
    "\n",
    "The RL agent explores different electrolysis settings.\n",
    "\n",
    "Uses Q-learning to update its policy based on rewards.\n",
    "\n",
    "After training, the RL model finds the best electrolysis conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad61d4a",
   "metadata": {},
   "source": [
    "# üåü Final Step: Testing the RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e2e61d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Electrolysis Setting: [2.1, 65, 3] | Reward: 23.7\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_52220/3389199018.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Optimal Electrolysis Setting: {state} | Reward: {reward}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "state = tuple(env.reset())\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space[np.argmax(Q_table[state])]\n",
    "    state, reward, done = env.step(action)\n",
    "    print(f\"Optimal Electrolysis Setting: {state} | Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142d2e6",
   "metadata": {},
   "source": [
    "üîπ This prints the best voltage, temperature, and pressure settings learned by AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a23a81",
   "metadata": {},
   "source": [
    "üöÄ Conclusion\n",
    "\n",
    "Reinforcement Learning enables AI to optimize hydrogen production dynamically.\n",
    "\n",
    "Q-learning helps AI adjust electrolysis parameters for best efficiency.\n",
    "\n",
    "This approach is crucial for achieving India's Green Hydrogen Goals! üåç‚ö°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad8eed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
